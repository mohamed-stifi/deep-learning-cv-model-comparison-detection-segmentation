{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["- dataset: **Penn-Fudan**\n","- Metrics: **Precision**, **Recall**, **mean Average Precision**\n","- Models: **Tow-stage detector** Mask R cnn, **Single-stage detector** Panoptic Fpn\n","- Optimaizers: **Adam**, **AdaGrad**, **RmsProp**"],"metadata":{"id":"xh9yFnwZVP-C"}},{"cell_type":"markdown","source":["# **Instance Segmentation with Optimizer Comparison**"],"metadata":{"id":"PSOrzU_QGai7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWia_ubWVJOR"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","from torchvision.transforms import functional as F\n","from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n","from torchvision.models.detection import MaskRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import cv2\n","from PIL import Image\n","import time\n","from tqdm import tqdm\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import urllib.request\n","import zipfile\n","import shutil\n","import seaborn as sns"]},{"cell_type":"markdown","source":["## **Data preprocessing and exploration**"],"metadata":{"id":"CziG0yhabVbr"}},{"cell_type":"code","source":["# Create output directory\n","OUTPUT_DIR = '/content/drive/MyDrive/DL-projet/results/instance_segmentation_results'\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n"],"metadata":{"id":"4VhnDCJ8GhIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def download_pennfudan_dataset():\n","    # Create a directory for the dataset\n","    os.makedirs(\"data\", exist_ok=True)\n","\n","    # URL for the Penn-Fudan Pedestrian dataset\n","    dataset_url = \"https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\"\n","    zip_path = \"data/PennFudanPed.zip\"\n","\n","    # Download the dataset\n","    if not os.path.exists(zip_path):\n","        print(f\"Downloading Penn-Fudan Pedestrian dataset from {dataset_url}...\")\n","        urllib.request.urlretrieve(dataset_url, zip_path)\n","        print(\"Download complete!\")\n","    else:\n","        print(\"Dataset zip already exists.\")\n","\n","    # Extract the dataset\n","    dataset_path = \"data/PennFudanPed\"\n","    if not os.path.exists(dataset_path):\n","        print(\"Extracting dataset...\")\n","        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","            zip_ref.extractall(\"data\")\n","        print(\"Extraction complete!\")\n","    else:\n","        print(\"Dataset already extracted.\")\n","\n","    # Verify the dataset structure\n","    if os.path.exists(os.path.join(dataset_path, \"PNGImages\")) and os.path.exists(os.path.join(dataset_path, \"PedMasks\")):\n","        print(\"Dataset is ready to use!\")\n","        print(f\"Number of images: {len(os.listdir(os.path.join(dataset_path, 'PNGImages')))}\")\n","        print(f\"Number of masks: {len(os.listdir(os.path.join(dataset_path, 'PedMasks')))}\")\n","        return dataset_path\n","    else:\n","        print(\"Dataset structure is not as expected. Please check the download.\")\n","        return None\n","\n","# Download the dataset\n","dataset_path = download_pennfudan_dataset()\n","print(f\"Dataset path: {dataset_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UAk-bVXI3fM","executionInfo":{"status":"ok","timestamp":1742852632166,"user_tz":0,"elapsed":2462,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}},"outputId":"a7a5cf09-2755-4b8f-c8bf-49a479905e45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading Penn-Fudan Pedestrian dataset from https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip...\n","Download complete!\n","Extracting dataset...\n","Extraction complete!\n","Dataset is ready to use!\n","Number of images: 170\n","Number of masks: 170\n","Dataset path: data/PennFudanPed\n"]}]},{"cell_type":"code","source":["# Custom dataset for Penn-Fudan\n","class PennFudanDataset(Dataset):\n","    def __init__(self, root, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        # Load all image files, sorting them to ensure they are aligned\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n","\n","    def __getitem__(self, idx):\n","        # Load images and masks\n","        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","        mask = Image.open(mask_path)\n","\n","        # Convert mask to numpy array\n","        mask = np.array(mask)\n","        # Instances are encoded as different colors\n","        obj_ids = np.unique(mask)\n","        # First id is the background, so remove it\n","        obj_ids = obj_ids[1:]\n","\n","        # Split the color-encoded mask into a set of binary masks\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # Get bounding box coordinates for each mask\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # There is only one class (pedestrian)\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # Suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)\n"],"metadata":{"id":"VU6GrxYRbO3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define transforms\n","def get_transform(train):\n","    transforms = []\n","    # Convert PIL image to tensor\n","    transforms.append(lambda img, target: (F.to_tensor(img), target))\n","    if train:\n","        # Random horizontal flip with 50% probability during training\n","        transforms.append(lambda img, target: random_horizontal_flip(img, target))\n","    return Compose(transforms)"],"metadata":{"id":"4zpnff57KjLH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper class for chaining transforms\n","class Compose:\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, image, target):\n","        for t in self.transforms:\n","            image, target = t(image, target)\n","        return image, target"],"metadata":{"id":"_vFaRfq0Kk-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random horizontal flip transform\n","def random_horizontal_flip(image, target, prob=0.5):\n","    if torch.rand(1) < prob:\n","        height, width = image.shape[-2:]\n","        image = image.flip(-1)\n","        bbox = target[\"boxes\"]\n","        # Flip boxes: xmin, ymin, xmax, ymax\n","        bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n","        target[\"boxes\"] = bbox\n","        if \"masks\" in target:\n","            target[\"masks\"] = target[\"masks\"].flip(-1)\n","    return image, target"],"metadata":{"id":"dk-zs5RAKk79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_maskrcnn_resnet18(num_classes):\n","    # Load pre-trained Mask R-CNN with ResNet50 backbone\n","    model = maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n","\n","    # Replace the backbone with ResNet18\n","    backbone = torchvision.models.resnet18(pretrained=True)\n","    # Remove the last fully connected layer\n","    backbone = nn.Sequential(*list(backbone.children())[:-2])\n","\n","    # Freeze all backbone parameters\n","    for param in backbone.parameters():\n","        param.requires_grad = False\n","\n","    # Get the number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # Replace the box predictor\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # Get the number of input features for the mask predictor\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # Replace the mask predictor\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n","\n","    # Freeze all parameters except for the prediction layers\n","    for name, param in model.named_parameters():\n","        # Only keep prediction layers trainable\n","        if not (\"box_predictor\" in name or \"mask_predictor\" in name or \"conv5_mask\" in name):\n","            param.requires_grad = False\n","\n","    # Verify which parameters are trainable\n","    trainable_params =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"Number of trainable parameters: {trainable_params}\")\n","\n","    return model"],"metadata":{"id":"SOr9N7LtKk57"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_panoptic_fpn(num_classes, num_stuff_classes=0, pretrained_backbone=True):\n","    # Load a pre-trained backbone with FPN\n","    backbone = resnet_fpn_backbone('resnet50', pretrained=pretrained_backbone)\n","\n","    # Freeze backbone layers\n","    for param in backbone.parameters():\n","        param.requires_grad = False\n","\n","    # RPN parameters\n","    anchor_generator = AnchorGenerator(\n","        sizes=((32,), (64,), (128,), (256,), (512,)), # Each size corresponds to a feature map\n","        aspect_ratios=((0.5, 1.0, 2.0),) * 5 # Repeat aspect ratios for each feature map\n","    )\n","\n","\n","    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","        featmap_names=['0', '1', '2', '3'],\n","        output_size=7,\n","        sampling_ratio=2\n","    )\n","\n","    mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","        featmap_names=['0', '1', '2', '3'],\n","        output_size=14,\n","        sampling_ratio=2\n","    )\n","\n","    # Create Mask R-CNN model with FPN backbone (base for Panoptic FPN)\n","    model = MaskRCNN(\n","        backbone,\n","        num_classes=num_classes,\n","        rpn_anchor_generator=anchor_generator,\n","        box_roi_pool=roi_pooler,\n","        mask_roi_pool=mask_roi_pooler\n","    )\n","\n","    # Modify the box predictor\n","    in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features_box, num_classes)\n","\n","    # Modify the mask predictor\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n","\n","    # For Panoptic FPN, we need to add semantic segmentation head for \"stuff\" classes\n","    if num_stuff_classes > 0:\n","        # Create semantic segmentation head\n","        model.semantic_seg_head = create_semantic_seg_head(backbone.out_channels, num_stuff_classes)\n","\n","    # Unfreeze specific layers for fine-tuning\n","    for name, param in model.named_parameters():\n","        if \"box_predictor\" in name or \"mask_predictor\" in name or \"semantic_seg_head\" in name:\n","            param.requires_grad = True\n","        else:\n","            param.requires_grad = False\n","\n","    # Print trainable parameters\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"Number of trainable parameters: {trainable_params}\")\n","\n","    return model\n","\n","def create_semantic_seg_head(in_channels, num_classes):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, 128, kernel_size=3, padding=1),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(128, num_classes, kernel_size=1)\n","    )"],"metadata":{"id":"a4g_-xfqwQuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training function\n","def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n","    model.train()\n","    metric_logger = MetricLogger(delimiter=\"  \")\n","    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n","    header = f'Epoch: [{epoch}]'\n","\n","    lr_scheduler = None\n","\n","    # Record gradients for visualization\n","    gradient_magnitudes = []\n","\n","    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        # Calculate gradient norms\n","        losses.backward()\n","        total_norm = 0\n","        for p in model.parameters():\n","            if p.grad is not None:\n","                param_norm = p.grad.data.norm(2)\n","                total_norm += param_norm.item() ** 2\n","        total_norm = total_norm ** 0.5\n","        gradient_magnitudes.append(total_norm)\n","\n","        optimizer.step()\n","\n","        metric_logger.update(loss=losses.item(), **{k: v.item() for k, v in loss_dict.items()})\n","        metric_logger.update(lr=optimizer.param_groups[0]['lr'])\n","\n","\n","\n","\n","    return metric_logger, gradient_magnitudes"],"metadata":{"id":"yEsfw_FsKk0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Utility class for averaging values\n","class SmoothedValue:\n","    def __init__(self, window_size=20, fmt=None):\n","        self.deque = []\n","        self.total = 0.0\n","        self.count = 0\n","        self.window_size = window_size\n","        self.fmt = fmt\n","\n","    def update(self, value, n=1):\n","        self.deque.append(value)\n","        if len(self.deque) > self.window_size:\n","            self.deque.pop(0)\n","        self.count += n\n","        self.total += value * n\n","\n","    @property\n","    def median(self):\n","        d = torch.tensor(self.deque)\n","        return d.median().item()\n","\n","    @property\n","    def avg(self):\n","        d = torch.tensor(self.deque)\n","        return d.mean().item()\n","\n","    @property\n","    def global_avg(self):\n","        return self.total / self.count\n","\n","    def __str__(self):\n","        if self.fmt is None:\n","            return self.global_avg\n","        return self.fmt.format(\n","            median=self.median,\n","            avg=self.avg,\n","            global_avg=self.global_avg)"],"metadata":{"id":"LE2Uk9_mMy-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Utility class for logging metrics\n","class MetricLogger:\n","    def __init__(self, delimiter=\"\\t\"):\n","        self.meters = {}\n","        self.delimiter = delimiter\n","\n","    def add_meter(self, name, meter): # Added add_meter method\n","        self.meters[name] = meter\n","\n","    def update(self, **kwargs):\n","        for k, v in kwargs.items():\n","            if k not in self.meters:\n","                self.meters[k] = SmoothedValue()\n","            self.meters[k].update(v)\n","\n","    def __str__(self):\n","        loss_str = []\n","        for name, meter in self.meters.items():\n","            loss_str.append(f\"{name}: {meter.global_avg:.4f}\")\n","        return self.delimiter.join(loss_str)\n","\n","    def log_every(self, iterable, print_freq, header=None):\n","        i = 0\n","        if header is not None:\n","            print(header)\n","        start_time = time.time()\n","        for obj in iterable:\n","            yield obj\n","            i += 1\n","            if i % print_freq == 0:\n","                print(f\"{i}/{len(iterable)}: {str(self)}\")\n","        print(f\"{i}/{len(iterable)}: {str(self)}\")\n","        total_time = time.time() - start_time\n","        print(f\"Total time: {total_time:.3f}, time per item: {total_time/len(iterable):.3f}\")"],"metadata":{"id":"cMx7v1mwMdGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, data_loader, device):\n","    model.eval()\n","    metric_logger = MetricLogger(delimiter=\"  \")\n","    header = 'Test:'\n","\n","    # Initialize metrics\n","    iou_list = []\n","    precision_list = []\n","    recall_list = []\n","    f1_list = []\n","    ap_scores = []\n","\n","    # IoU threshold for positive detection\n","    iou_threshold = 0.5\n","\n","    for images, targets in metric_logger.log_every(data_loader, 10, header):\n","        images = list(img.to(device) for img in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        with torch.no_grad():\n","            outputs = model(images)\n","\n","        # Process each image in the batch\n","        for i, (output, target) in enumerate(zip(outputs, targets)):\n","            # Get predicted masks and scores\n","            pred_masks = output['masks']\n","            pred_scores = output['scores']\n","            pred_labels = output['labels']\n","\n","            # Threshold predictions by score\n","            score_threshold = 0.5\n","            keep = pred_scores > score_threshold\n","            pred_masks = pred_masks[keep]\n","            pred_scores = pred_scores[keep]\n","            pred_labels = pred_labels[keep]\n","\n","            # Get target masks and labels\n","            gt_masks = target['masks']\n","            gt_labels = target['labels']\n","\n","            # Skip images with no ground truth instances\n","            if len(gt_masks) == 0:\n","                continue\n","\n","            # Skip images with no predictions\n","            if len(pred_masks) == 0:\n","                # Add zeros to metrics for this image\n","                precision_list.append(0)\n","                recall_list.append(0)\n","                f1_list.append(0)\n","                continue\n","\n","            # Calculate IoU between each prediction and ground truth pair\n","            ious = torch.zeros((len(pred_masks), len(gt_masks)), device=device)\n","            pred_areas = pred_masks.sum((1, 2, 3))\n","            gt_areas = gt_masks.sum((1, 2))\n","\n","            # Calculate IoU for each pred-gt pair\n","            for p_idx, p_mask in enumerate(pred_masks):\n","                p_mask = p_mask.squeeze(1) > 0.5  # Convert to binary mask\n","                for gt_idx, gt_mask in enumerate(gt_masks):\n","                    # Skip if labels don't match (for multi-class)\n","                    if pred_labels[p_idx] != gt_labels[gt_idx]:\n","                        continue\n","\n","                    intersection = (p_mask & gt_mask).float().sum()\n","                    union = pred_areas[p_idx] + gt_areas[gt_idx] - intersection\n","                    if union > 0:\n","                        ious[p_idx, gt_idx] = intersection / union\n","\n","            # Calculate metrics\n","            # For each ground truth, find the prediction with highest IoU\n","            matched_ious = []\n","            matched_indices = set()\n","\n","            if ious.numel() > 0:\n","                # For each ground truth, find best matching prediction\n","                for gt_idx in range(len(gt_masks)):\n","                    if ious.shape[0] > 0:  # Check if there are any predictions\n","                        best_pred_idx = ious[:, gt_idx].argmax()\n","                        best_iou = ious[best_pred_idx, gt_idx]\n","\n","                        # Only count as match if IoU exceeds threshold\n","                        if best_iou >= iou_threshold and best_pred_idx not in matched_indices:\n","                            matched_ious.append(best_iou.item())\n","                            matched_indices.add(best_pred_idx)\n","\n","            # Calculate precision, recall, F1\n","            if len(pred_masks) > 0:\n","                precision = len(matched_indices) / len(pred_masks)\n","                precision_list.append(precision)\n","            else:\n","                precision_list.append(0)\n","\n","            if len(gt_masks) > 0:\n","                recall = len(matched_indices) / len(gt_masks)\n","                recall_list.append(recall)\n","            else:\n","                recall_list.append(0)\n","\n","            # F1 Score\n","            if precision + recall > 0:\n","                f1 = 2 * precision * recall / (precision + recall)\n","                f1_list.append(f1)\n","            else:\n","                f1_list.append(0)\n","\n","            # Record IoUs\n","            if matched_ious:\n","                iou_list.extend(matched_ious)\n","\n","            # Calculate AP for this image (for mAP)\n","            # Sort predictions by score\n","            sorted_indices = torch.argsort(pred_scores, descending=True)\n","            sorted_masks = pred_masks[sorted_indices]\n","            sorted_scores = pred_scores[sorted_indices]\n","            sorted_labels = pred_labels[sorted_indices]\n","\n","            # Calculate precision at different recall points\n","            tp = torch.zeros(len(sorted_masks))\n","            fp = torch.zeros(len(sorted_masks))\n","            gt_matched = set()\n","\n","            for p_idx, p_mask in enumerate(sorted_masks):\n","                p_mask = p_mask.squeeze(1) > 0.5\n","                p_label = sorted_labels[p_idx]\n","\n","                max_iou = 0\n","                max_gt_idx = -1\n","\n","                # Find the GT with highest IoU\n","                for gt_idx, gt_mask in enumerate(gt_masks):\n","                    if gt_idx in gt_matched or gt_labels[gt_idx] != p_label:\n","                        continue\n","\n","                    intersection = (p_mask & gt_mask).float().sum()\n","                    union = p_mask.float().sum() + gt_mask.float().sum() - intersection\n","                    iou = intersection / union if union > 0 else 0\n","\n","                    if iou > max_iou:\n","                        max_iou = iou\n","                        max_gt_idx = gt_idx\n","\n","                # Check if match is valid\n","                if max_iou >= iou_threshold and max_gt_idx not in gt_matched:\n","                    tp[p_idx] = 1\n","                    gt_matched.add(max_gt_idx)\n","                else:\n","                    fp[p_idx] = 1\n","\n","            # Calculate cumulative precision and recall\n","            cum_tp = torch.cumsum(tp, dim=0)\n","            cum_fp = torch.cumsum(fp, dim=0)\n","\n","            cum_precision = cum_tp / (cum_tp + cum_fp)\n","            cum_recall = cum_tp / len(gt_masks) if len(gt_masks) > 0 else cum_tp * 0\n","\n","            # Calculate AP using 11-point interpolation\n","            ap = 0\n","            for t in torch.arange(0, 1.1, 0.1):\n","                if torch.sum(cum_recall >= t) == 0:\n","                    p = 0\n","                else:\n","                    p = torch.max(cum_precision[cum_recall >= t])\n","                ap = ap + p / 11\n","\n","            ap_scores.append(ap.item())\n","\n","    # Aggregate results\n","    results = {}\n","\n","    # Mean IoU\n","    if iou_list:\n","        results['mIoU'] = sum(iou_list) / len(iou_list)\n","    else:\n","        results['mIoU'] = 0\n","\n","    # Mean Precision\n","    if precision_list:\n","        results['Precision'] = sum(precision_list) / len(precision_list)\n","    else:\n","        results['Precision'] = 0\n","\n","    # Mean Recall\n","    if recall_list:\n","        results['Recall'] = sum(recall_list) / len(recall_list)\n","    else:\n","        results['Recall'] = 0\n","\n","    # Mean F1\n","    if f1_list:\n","        results['F1'] = sum(f1_list) / len(f1_list)\n","    else:\n","        results['F1'] = 0\n","\n","    # Mean Average Precision (mAP)\n","    if ap_scores:\n","        results['mAP'] = sum(ap_scores) / len(ap_scores)\n","    else:\n","        results['mAP'] = 0\n","\n","    print(f\"\\nEvaluation Results:\")\n","    for metric, value in results.items():\n","        print(f\"{metric}: {value:.4f}\")\n","\n","    return results"],"metadata":{"id":"K5CsAgGzKkxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compare_optimizers(model_name, train_loader, val_loader, device, num_epochs=5):\n","    optimizers = {\n","        'Adam': None,\n","        'AdaGrad': None,\n","        'RMSProp': None\n","    }\n","    results = {}\n","    gradient_data = {}\n","    learning_rates = {\n","        'Adam': 0.001,\n","        'AdaGrad': 0.01,\n","        'RMSProp': 0.001\n","    }\n","\n","    for opt_name in optimizers.keys():\n","        print(f\"\\nTraining with {opt_name} optimizer\")\n","        if model_name == 'maskrcnn':\n","            model = get_maskrcnn_resnet18(num_classes=2)  # Background + pedestrian\n","            model.to(device)\n","\n","        elif model_name == 'panoptic':\n","            model = get_panoptic_fpn(num_classes=2)  # Background + pedestrian\n","            model.to(device)\n","\n","        if opt_name == 'Adam':\n","            optimizer = optim.Adam(model.parameters(), lr=learning_rates[opt_name])\n","            optimizers[opt_name] = optimizer\n","        elif opt_name == 'AdaGrad':\n","            optimizer = optim.Adagrad(model.parameters(), lr=learning_rates[opt_name])\n","            optimizers[opt_name] = optimizer\n","        elif opt_name == 'RMSProp':\n","            optimizer = optim.RMSprop(model.parameters(), lr=learning_rates[opt_name])\n","            optimizers[opt_name] = optimizer\n","\n","        # Training loop\n","        epoch_losses = []\n","        epoch_gradients = []\n","        epoch_lr = []\n","\n","        for epoch in range(num_epochs):\n","            metric_logger, gradients = train_one_epoch(\n","                model, optimizer, train_loader, device, epoch)\n","\n","            epoch_losses.append(metric_logger.meters['loss'].global_avg)\n","            epoch_gradients.append(sum(gradients) / len(gradients))\n","            epoch_lr.append(optimizer.param_groups[0]['lr'])\n","\n","            # Evaluate model\n","            eval_results = evaluate(model, val_loader, device)\n","            print(f\"Epoch {epoch}: Loss = {epoch_losses[-1]:.4f}\")\n","\n","\n","        results[opt_name] = {\n","            'losses': epoch_losses,\n","            'final_metrics': evaluate(model, val_loader, device),\n","            'learning_rates': epoch_lr\n","        }\n","\n","        gradient_data[opt_name] = {\n","            'gradients': epoch_gradients,\n","            'learning_rates': epoch_lr\n","        }\n","\n","    return results, gradient_data, model\n","\n","def visualize_comprehensive_plots(results, gradient_data, output_dir, model_name, optim_name):\n","    \"\"\"Create comprehensive visualizations for optimizer performance.\"\"\"\n","\n","    # Set up plotting style\n","    # plt.style.use('seaborn')\n","\n","    # 1. Loss Comparison\n","    plt.figure(figsize=(15, 10))\n","    plt.subplot(2, 2, 1)\n","    for opt_name, res in results.items():\n","        plt.plot(res['losses'], label=f\"{opt_name}\")\n","    plt.title('Training Loss by Optimizer', fontsize=12)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # 2. Gradient Magnitudes\n","    plt.subplot(2, 2, 2)\n","    for opt_name, data in gradient_data.items():\n","        plt.plot(data['gradients'], label=f\"{opt_name}\")\n","    plt.title('Gradient Magnitudes by Optimizer', fontsize=12)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Average Gradient Magnitude')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # 3. Learning Rate vs Loss\n","    plt.subplot(2, 2, 3)\n","    for opt_name, res in results.items():\n","        plt.plot(res['learning_rates'], res['losses'], label=f\"{opt_name}\")\n","    plt.title('Learning Rate vs Loss', fontsize=12)\n","    plt.xlabel('Learning Rate')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # 4. Metric Comparison\n","    plt.subplot(2, 2, 4)\n","    metrics = ['mIoU', 'Precision', 'Recall', 'F1', 'mAP']\n","    optimizer_names = list(results.keys())\n","    metric_values = np.array([\n","        [results[opt]['final_metrics'][m] for m in metrics]\n","        for opt in optimizer_names\n","    ])\n","\n","    sns.heatmap(metric_values, annot=True, cmap='YlGnBu',\n","                xticklabels=metrics, yticklabels=optimizer_names)\n","    plt.title('Optimizer Performance Metrics', fontsize=12)\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(output_dir, f'{model_name}_{optim_name}_optimizer_comprehensive_analysis.png'), dpi=300)\n","    plt.close()\n","\n","    # Detailed Bar Plots for Each Metric\n","    for metric in metrics:\n","        plt.figure(figsize=(10, 6))\n","        metric_data = [results[opt]['final_metrics'][metric] for opt in optimizer_names]\n","\n","        plt.bar(optimizer_names, metric_data, color=['blue', 'green', 'red'])\n","        plt.title(f'{metric} Comparison Across Optimizers', fontsize=12)\n","        plt.ylabel(metric)\n","        plt.ylim(0, max(metric_data) * 1.2)\n","\n","        for i, v in enumerate(metric_data):\n","            plt.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n","\n","        plt.tight_layout()\n","\n","        plt.savefig(os.path.join(output_dir, f'{model_name}_{optim_name}_optimizer_{metric.lower()}_comparison.png'), dpi=300)\n","        plt.close()"],"metadata":{"id":"qgs8T7PLKkvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize results\n","def visualize_optimizer_comparison(results, gradient_data, model_name, optim_name):\n","    # Plot loss curves\n","    plt.figure(figsize=(12, 6))\n","    for opt_name, res in results.items():\n","        plt.plot(res['losses'], label=f\"{opt_name}\")\n","    plt.title('Training Loss by Optimizer')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f'{model_name}_{optim_name}_optimizer_loss_comparison.png')\n","    plt.close()\n","\n","    # Plot gradient magnitudes\n","    plt.figure(figsize=(12, 6))\n","    for opt_name, grads in gradient_data.items():\n","        plt.plot(grads, label=f\"{opt_name}\")\n","    plt.title('Gradient Magnitudes by Optimizer')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Average Gradient Magnitude')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f'{model_name}_{optim_name}_optimizer_gradient_comparison.png')\n","    plt.close()\n","\n","# Visualize segmentation results\n","def visualize_predictions(model, dataset, model_name, optim_name,idx=0, device='cpu'):\n","    img, target = dataset[idx]\n","    model.eval()\n","    with torch.no_grad():\n","        prediction = model([img.to(device)])[0]\n","\n","    # Convert tensors to numpy arrays\n","    img = img.mul(255).permute(1, 2, 0).byte().numpy()\n","\n","    # Draw ground truth\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n","\n","    # Ground truth\n","    ax1.imshow(img)\n","    for mask in target['masks']:\n","        mask = mask.numpy()\n","        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","        for contour in contours:\n","            contour = contour.reshape(-1, 2)\n","            ax1.plot(contour[:, 0], contour[:, 1], linewidth=2)\n","    ax1.set_title('Ground Truth')\n","    ax1.axis('off')\n","\n","    # Predictions\n","    ax2.imshow(img)\n","    for mask in prediction['masks']:\n","        if mask.max() > 0.5:  # Threshold\n","            mask = mask.squeeze().cpu().numpy() > 0.5\n","            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","            for contour in contours:\n","                contour = contour.reshape(-1, 2)\n","                ax2.plot(contour[:, 0], contour[:, 1], linewidth=2)\n","    ax2.set_title('Predictions')\n","    ax2.axis('off')\n","\n","    plt.tight_layout()\n","    plt.savefig(f'{model_name}_{optim_name}_segmentation_comparison.png')\n","    plt.close()"],"metadata":{"id":"9MxenyabNd_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate metrics\n","def calculate_metrics(model, data_loader, device):\n","    model.eval()\n","    metrics = {\n","        'iou': [],\n","        'precision': [],\n","        'recall': [],\n","        'f1': []\n","    }\n","\n","    for images, targets in data_loader:\n","        images = list(img.to(device) for img in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        with torch.no_grad():\n","            outputs = model(images)\n","\n","        # Calculate metrics for each image\n","        for i, (output, target) in enumerate(zip(outputs, targets)):\n","            # Check if 'masks' are present in the output\n","            if 'masks' not in output:\n","                print(\"Warning: 'masks' not found in output. Skipping metric calculation for this image.\")\n","                continue  # Skip to the next image\n","\n","            pred_masks = output['masks'] > 0.5\n","            gt_masks = target['masks']\n","\n","            for j, pred_mask in enumerate(pred_masks):\n","                best_iou = 0\n","                best_precision = 0\n","                best_recall = 0\n","\n","                for gt_mask in gt_masks:\n","                    # Calculate IoU\n","                    intersection = torch.logical_and(pred_mask, gt_mask).sum().float()\n","                    union = torch.logical_or(pred_mask, gt_mask).sum().float()\n","                    # Convert to float if necessary\n","                    iou = intersection / union if union > 0 else 0.0 # Changed to float\n","\n","                    # Calculate precision and recall\n","                    tp = intersection\n","                    fp = pred_mask.sum() - tp\n","                    fn = gt_mask.sum() - tp\n","\n","                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","\n","                    if iou > best_iou:\n","                        best_iou = iou\n","                        best_precision = precision\n","                        best_recall = recall\n","\n","                # Calculate F1 score\n","                f1 = 2 * best_precision * best_recall / (best_precision + best_recall) if (best_precision + best_recall) > 0 else 0\n","\n","                # Call item() only if it's a Tensor\n","                metrics['iou'].append(best_iou.item() if isinstance(best_iou, torch.Tensor) else best_iou) # add condition to check type before calling item()\n","                metrics['precision'].append(best_precision.item() if isinstance(best_precision, torch.Tensor) else best_precision) # add condition to check type before calling item()\n","                metrics['recall'].append(best_recall.item() if isinstance(best_recall, torch.Tensor) else best_recall) # add condition to check type before calling item()\n","                metrics['f1'].append(f1.item() if isinstance(f1, torch.Tensor) else f1) # add condition to check type before calling item()\n","\n","    # Average the metrics\n","    for k in metrics:\n","        if metrics[k]:\n","            metrics[k] = sum(metrics[k]) / len(metrics[k])\n","        else:\n","            metrics[k] = 0\n","\n","    return metrics"],"metadata":{"id":"D7U3l694N8o1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Set device\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    print(f\"Using device: {device}\")\n","\n","    # Set paths\n","    data_path = \"data/PennFudanPed\"\n","\n","    # Create dataset\n","    dataset = PennFudanDataset(data_path, get_transform(train=True))\n","    dataset_test = PennFudanDataset(data_path, get_transform(train=False))\n","\n","    # Split the dataset into train and validation\n","    indices = torch.randperm(len(dataset)).tolist()\n","    train_size = int(len(dataset) * 0.8)\n","    dataset_train = torch.utils.data.Subset(dataset, indices[:train_size])\n","    dataset_val = torch.utils.data.Subset(dataset_test, indices[train_size:])\n","\n","    # Create data loaders\n","    train_loader = DataLoader(\n","        dataset_train, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x))\n","    )\n","    val_loader = DataLoader(\n","        dataset_val, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x))\n","    )\n","\n","    # Compare optimizers for Mask R-CNN\n","    print(\"Training Mask R-CNN with ResNet18 backbone\")\n","    maskrcnn_results, maskrcnn_gradients, model_maskrcnn = compare_optimizers(\n","        'maskrcnn', train_loader, val_loader, device, num_epochs=5\n","    )\n","\n","    # Compare optimizers for Panoptic FPN\n","    print(\"\\nTraining Panoptic FPN\")\n","    panoptic_results, panoptic_gradients, model_panoptic = compare_optimizers(\n","        'panoptic', train_loader, val_loader, device, num_epochs=5\n","    )\n","\n","    # Comprehensive Visualization\n","    visualize_comprehensive_plots(maskrcnn_results, maskrcnn_gradients, OUTPUT_DIR, 'maskrcnn', '')\n","    visualize_comprehensive_plots(panoptic_results, panoptic_gradients, OUTPUT_DIR, 'panoptic', '')\n","\n","    # Save detailed results to CSV\n","    results_df_maskrcnn = pd.DataFrame([\n","        {**{'Model': 'Mask R-CNN', 'Optimizer': opt}, **metrics}\n","        for opt, metrics in maskrcnn_results.items()\n","    ])\n","    results_df_panoptic = pd.DataFrame([\n","        {**{'Model': 'Panoptic FPN', 'Optimizer': opt}, **metrics}\n","        for opt, metrics in panoptic_results.items()\n","    ])\n","\n","    results_df = pd.concat([results_df_maskrcnn, results_df_panoptic])\n","    results_df.to_csv(os.path.join(OUTPUT_DIR, 'model_optimizer_comparison.csv'), index=False)\n","\n","    print(f\"\\nResults and visualizations saved to {OUTPUT_DIR}\")\n"],"metadata":{"id":"9WvqGMC8KksS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"Pp2N490RKkph","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5ab383e0-0256-40b9-eca5-ec09359ca2a4","executionInfo":{"status":"ok","timestamp":1742856905341,"user_tz":0,"elapsed":756391,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training Mask R-CNN with ResNet18 backbone\n","\n","Training with Adam optimizer\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Number of trainable parameters: 273164\n","Epoch: [0]\n","10/68: lr: 0.0010  loss: 1.4644  loss_classifier: 0.3404  loss_box_reg: 0.2015  loss_mask: 0.8959  loss_objectness: 0.0227  loss_rpn_box_reg: 0.0038\n","20/68: lr: 0.0010  loss: 1.1729  loss_classifier: 0.2907  loss_box_reg: 0.2024  loss_mask: 0.6574  loss_objectness: 0.0183  loss_rpn_box_reg: 0.0041\n","30/68: lr: 0.0010  loss: 0.9884  loss_classifier: 0.2300  loss_box_reg: 0.1703  loss_mask: 0.5661  loss_objectness: 0.0180  loss_rpn_box_reg: 0.0040\n","40/68: lr: 0.0010  loss: 0.8890  loss_classifier: 0.1931  loss_box_reg: 0.1541  loss_mask: 0.5164  loss_objectness: 0.0203  loss_rpn_box_reg: 0.0052\n","50/68: lr: 0.0010  loss: 0.8045  loss_classifier: 0.1684  loss_box_reg: 0.1431  loss_mask: 0.4669  loss_objectness: 0.0211  loss_rpn_box_reg: 0.0050\n","60/68: lr: 0.0010  loss: 0.7298  loss_classifier: 0.1497  loss_box_reg: 0.1325  loss_mask: 0.4227  loss_objectness: 0.0198  loss_rpn_box_reg: 0.0050\n","68/68: lr: 0.0010  loss: 0.6760  loss_classifier: 0.1370  loss_box_reg: 0.1232  loss_mask: 0.3911  loss_objectness: 0.0197  loss_rpn_box_reg: 0.0050\n","Total time: 19.996, time per item: 0.294\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.480, time per item: 0.132\n","\n","Evaluation Results:\n","mIoU: 0.8675\n","Precision: 0.7436\n","Recall: 1.0000\n","F1: 0.8311\n","mAP: 0.9803\n","Epoch 0: Loss = 0.6760\n","Epoch: [1]\n","10/68: lr: 0.0010  loss: 0.2985  loss_classifier: 0.0494  loss_box_reg: 0.0676  loss_mask: 0.1578  loss_objectness: 0.0183  loss_rpn_box_reg: 0.0054\n","20/68: lr: 0.0010  loss: 0.3065  loss_classifier: 0.0499  loss_box_reg: 0.0710  loss_mask: 0.1594  loss_objectness: 0.0212  loss_rpn_box_reg: 0.0050\n","30/68: lr: 0.0010  loss: 0.3117  loss_classifier: 0.0497  loss_box_reg: 0.0704  loss_mask: 0.1623  loss_objectness: 0.0241  loss_rpn_box_reg: 0.0052\n","40/68: lr: 0.0010  loss: 0.3091  loss_classifier: 0.0480  loss_box_reg: 0.0683  loss_mask: 0.1650  loss_objectness: 0.0226  loss_rpn_box_reg: 0.0052\n","50/68: lr: 0.0010  loss: 0.3112  loss_classifier: 0.0466  loss_box_reg: 0.0679  loss_mask: 0.1698  loss_objectness: 0.0219  loss_rpn_box_reg: 0.0050\n","60/68: lr: 0.0010  loss: 0.3088  loss_classifier: 0.0468  loss_box_reg: 0.0669  loss_mask: 0.1676  loss_objectness: 0.0226  loss_rpn_box_reg: 0.0049\n","68/68: lr: 0.0010  loss: 0.3134  loss_classifier: 0.0476  loss_box_reg: 0.0697  loss_mask: 0.1682  loss_objectness: 0.0229  loss_rpn_box_reg: 0.0050\n","Total time: 20.376, time per item: 0.300\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.350, time per item: 0.128\n","\n","Evaluation Results:\n","mIoU: 0.8718\n","Precision: 0.7401\n","Recall: 1.0000\n","F1: 0.8279\n","mAP: 0.9846\n","Epoch 1: Loss = 0.3134\n","Epoch: [2]\n","10/68: lr: 0.0010  loss: 0.3292  loss_classifier: 0.0492  loss_box_reg: 0.0782  loss_mask: 0.1678  loss_objectness: 0.0271  loss_rpn_box_reg: 0.0069\n","20/68: lr: 0.0010  loss: 0.3128  loss_classifier: 0.0489  loss_box_reg: 0.0780  loss_mask: 0.1545  loss_objectness: 0.0253  loss_rpn_box_reg: 0.0061\n","30/68: lr: 0.0010  loss: 0.2960  loss_classifier: 0.0451  loss_box_reg: 0.0710  loss_mask: 0.1499  loss_objectness: 0.0247  loss_rpn_box_reg: 0.0053\n","40/68: lr: 0.0010  loss: 0.2896  loss_classifier: 0.0428  loss_box_reg: 0.0663  loss_mask: 0.1513  loss_objectness: 0.0240  loss_rpn_box_reg: 0.0051\n","50/68: lr: 0.0010  loss: 0.2887  loss_classifier: 0.0420  loss_box_reg: 0.0659  loss_mask: 0.1513  loss_objectness: 0.0244  loss_rpn_box_reg: 0.0050\n","60/68: lr: 0.0010  loss: 0.2900  loss_classifier: 0.0424  loss_box_reg: 0.0671  loss_mask: 0.1511  loss_objectness: 0.0244  loss_rpn_box_reg: 0.0051\n","68/68: lr: 0.0010  loss: 0.2928  loss_classifier: 0.0423  loss_box_reg: 0.0665  loss_mask: 0.1538  loss_objectness: 0.0253  loss_rpn_box_reg: 0.0050\n","Total time: 20.104, time per item: 0.296\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.288, time per item: 0.126\n","\n","Evaluation Results:\n","mIoU: 0.8662\n","Precision: 0.7445\n","Recall: 1.0000\n","F1: 0.8310\n","mAP: 0.9846\n","Epoch 2: Loss = 0.2928\n","Epoch: [3]\n","10/68: lr: 0.0010  loss: 0.2764  loss_classifier: 0.0379  loss_box_reg: 0.0596  loss_mask: 0.1463  loss_objectness: 0.0263  loss_rpn_box_reg: 0.0062\n","20/68: lr: 0.0010  loss: 0.2659  loss_classifier: 0.0342  loss_box_reg: 0.0574  loss_mask: 0.1499  loss_objectness: 0.0194  loss_rpn_box_reg: 0.0050\n","30/68: lr: 0.0010  loss: 0.2769  loss_classifier: 0.0372  loss_box_reg: 0.0613  loss_mask: 0.1520  loss_objectness: 0.0213  loss_rpn_box_reg: 0.0051\n","40/68: lr: 0.0010  loss: 0.2783  loss_classifier: 0.0377  loss_box_reg: 0.0621  loss_mask: 0.1534  loss_objectness: 0.0200  loss_rpn_box_reg: 0.0051\n","50/68: lr: 0.0010  loss: 0.2723  loss_classifier: 0.0364  loss_box_reg: 0.0588  loss_mask: 0.1525  loss_objectness: 0.0199  loss_rpn_box_reg: 0.0048\n","60/68: lr: 0.0010  loss: 0.2853  loss_classifier: 0.0395  loss_box_reg: 0.0637  loss_mask: 0.1560  loss_objectness: 0.0207  loss_rpn_box_reg: 0.0053\n","68/68: lr: 0.0010  loss: 0.2854  loss_classifier: 0.0387  loss_box_reg: 0.0641  loss_mask: 0.1573  loss_objectness: 0.0202  loss_rpn_box_reg: 0.0052\n","Total time: 19.615, time per item: 0.288\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.412, time per item: 0.130\n","\n","Evaluation Results:\n","mIoU: 0.8578\n","Precision: 0.7604\n","Recall: 1.0000\n","F1: 0.8432\n","mAP: 0.9846\n","Epoch 3: Loss = 0.2854\n","Epoch: [4]\n","10/68: lr: 0.0010  loss: 0.3626  loss_classifier: 0.0475  loss_box_reg: 0.0891  loss_mask: 0.1978  loss_objectness: 0.0211  loss_rpn_box_reg: 0.0071\n","20/68: lr: 0.0010  loss: 0.3218  loss_classifier: 0.0405  loss_box_reg: 0.0760  loss_mask: 0.1799  loss_objectness: 0.0187  loss_rpn_box_reg: 0.0067\n","30/68: lr: 0.0010  loss: 0.3146  loss_classifier: 0.0408  loss_box_reg: 0.0696  loss_mask: 0.1775  loss_objectness: 0.0204  loss_rpn_box_reg: 0.0063\n","40/68: lr: 0.0010  loss: 0.2967  loss_classifier: 0.0378  loss_box_reg: 0.0625  loss_mask: 0.1709  loss_objectness: 0.0201  loss_rpn_box_reg: 0.0054\n","50/68: lr: 0.0010  loss: 0.2928  loss_classifier: 0.0380  loss_box_reg: 0.0611  loss_mask: 0.1677  loss_objectness: 0.0209  loss_rpn_box_reg: 0.0052\n","60/68: lr: 0.0010  loss: 0.2903  loss_classifier: 0.0380  loss_box_reg: 0.0623  loss_mask: 0.1650  loss_objectness: 0.0195  loss_rpn_box_reg: 0.0054\n","68/68: lr: 0.0010  loss: 0.2861  loss_classifier: 0.0374  loss_box_reg: 0.0613  loss_mask: 0.1623  loss_objectness: 0.0199  loss_rpn_box_reg: 0.0052\n","Total time: 19.729, time per item: 0.290\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.630, time per item: 0.136\n","\n","Evaluation Results:\n","mIoU: 0.8716\n","Precision: 0.7374\n","Recall: 1.0000\n","F1: 0.8278\n","mAP: 0.9938\n","Epoch 4: Loss = 0.2861\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.337, time per item: 0.128\n","\n","Evaluation Results:\n","mIoU: 0.8716\n","Precision: 0.7374\n","Recall: 1.0000\n","F1: 0.8278\n","mAP: 0.9938\n","\n","Training with AdaGrad optimizer\n","Number of trainable parameters: 273164\n","Epoch: [0]\n","10/68: lr: 0.0100  loss: 8.2255  loss_classifier: 0.2217  loss_box_reg: 0.2491  loss_mask: 7.7217  loss_objectness: 0.0265  loss_rpn_box_reg: 0.0065\n","20/68: lr: 0.0100  loss: 4.4355  loss_classifier: 0.1443  loss_box_reg: 0.2032  loss_mask: 4.0567  loss_objectness: 0.0255  loss_rpn_box_reg: 0.0058\n","30/68: lr: 0.0100  loss: 3.1111  loss_classifier: 0.1124  loss_box_reg: 0.1638  loss_mask: 2.8047  loss_objectness: 0.0251  loss_rpn_box_reg: 0.0051\n","40/68: lr: 0.0100  loss: 2.4426  loss_classifier: 0.0952  loss_box_reg: 0.1406  loss_mask: 2.1789  loss_objectness: 0.0233  loss_rpn_box_reg: 0.0047\n","50/68: lr: 0.0100  loss: 2.0449  loss_classifier: 0.0876  loss_box_reg: 0.1335  loss_mask: 1.7972  loss_objectness: 0.0218  loss_rpn_box_reg: 0.0048\n","60/68: lr: 0.0100  loss: 1.7662  loss_classifier: 0.0810  loss_box_reg: 0.1267  loss_mask: 1.5327  loss_objectness: 0.0209  loss_rpn_box_reg: 0.0049\n","68/68: lr: 0.0100  loss: 1.5973  loss_classifier: 0.0775  loss_box_reg: 0.1210  loss_mask: 1.3729  loss_objectness: 0.0208  loss_rpn_box_reg: 0.0051\n","Total time: 20.067, time per item: 0.295\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.511, time per item: 0.133\n","\n","Evaluation Results:\n","mIoU: 0.8680\n","Precision: 0.7436\n","Recall: 1.0000\n","F1: 0.8301\n","mAP: 0.9649\n","Epoch 0: Loss = 1.5973\n","Epoch: [1]\n","10/68: lr: 0.0100  loss: 0.3964  loss_classifier: 0.0567  loss_box_reg: 0.1116  loss_mask: 0.2005  loss_objectness: 0.0212  loss_rpn_box_reg: 0.0064\n","20/68: lr: 0.0100  loss: 0.3604  loss_classifier: 0.0468  loss_box_reg: 0.0892  loss_mask: 0.1988  loss_objectness: 0.0201  loss_rpn_box_reg: 0.0055\n","30/68: lr: 0.0100  loss: 0.3396  loss_classifier: 0.0449  loss_box_reg: 0.0817  loss_mask: 0.1871  loss_objectness: 0.0208  loss_rpn_box_reg: 0.0051\n","40/68: lr: 0.0100  loss: 0.3247  loss_classifier: 0.0418  loss_box_reg: 0.0752  loss_mask: 0.1845  loss_objectness: 0.0188  loss_rpn_box_reg: 0.0045\n","50/68: lr: 0.0100  loss: 0.3220  loss_classifier: 0.0417  loss_box_reg: 0.0748  loss_mask: 0.1808  loss_objectness: 0.0201  loss_rpn_box_reg: 0.0045\n","60/68: lr: 0.0100  loss: 0.3181  loss_classifier: 0.0417  loss_box_reg: 0.0743  loss_mask: 0.1785  loss_objectness: 0.0190  loss_rpn_box_reg: 0.0047\n","68/68: lr: 0.0100  loss: 0.3215  loss_classifier: 0.0429  loss_box_reg: 0.0761  loss_mask: 0.1781  loss_objectness: 0.0195  loss_rpn_box_reg: 0.0050\n","Total time: 19.799, time per item: 0.291\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.557, time per item: 0.134\n","\n","Evaluation Results:\n","mIoU: 0.8706\n","Precision: 0.7419\n","Recall: 1.0000\n","F1: 0.8286\n","mAP: 0.9744\n","Epoch 1: Loss = 0.3215\n","Epoch: [2]\n","10/68: lr: 0.0100  loss: 0.3642  loss_classifier: 0.0488  loss_box_reg: 0.0928  loss_mask: 0.1842  loss_objectness: 0.0316  loss_rpn_box_reg: 0.0067\n","20/68: lr: 0.0100  loss: 0.3213  loss_classifier: 0.0387  loss_box_reg: 0.0786  loss_mask: 0.1719  loss_objectness: 0.0263  loss_rpn_box_reg: 0.0058\n","30/68: lr: 0.0100  loss: 0.3008  loss_classifier: 0.0373  loss_box_reg: 0.0691  loss_mask: 0.1636  loss_objectness: 0.0262  loss_rpn_box_reg: 0.0046\n","40/68: lr: 0.0100  loss: 0.2991  loss_classifier: 0.0384  loss_box_reg: 0.0701  loss_mask: 0.1609  loss_objectness: 0.0248  loss_rpn_box_reg: 0.0049\n","50/68: lr: 0.0100  loss: 0.3039  loss_classifier: 0.0392  loss_box_reg: 0.0684  loss_mask: 0.1673  loss_objectness: 0.0241  loss_rpn_box_reg: 0.0049\n","60/68: lr: 0.0100  loss: 0.3020  loss_classifier: 0.0386  loss_box_reg: 0.0683  loss_mask: 0.1659  loss_objectness: 0.0244  loss_rpn_box_reg: 0.0048\n","68/68: lr: 0.0100  loss: 0.2978  loss_classifier: 0.0382  loss_box_reg: 0.0671  loss_mask: 0.1632  loss_objectness: 0.0243  loss_rpn_box_reg: 0.0050\n","Total time: 19.551, time per item: 0.288\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.476, time per item: 0.132\n","\n","Evaluation Results:\n","mIoU: 0.8754\n","Precision: 0.7678\n","Recall: 1.0000\n","F1: 0.8471\n","mAP: 0.9891\n","Epoch 2: Loss = 0.2978\n","Epoch: [3]\n","10/68: lr: 0.0100  loss: 0.2728  loss_classifier: 0.0398  loss_box_reg: 0.0605  loss_mask: 0.1471  loss_objectness: 0.0215  loss_rpn_box_reg: 0.0039\n","20/68: lr: 0.0100  loss: 0.2746  loss_classifier: 0.0388  loss_box_reg: 0.0594  loss_mask: 0.1513  loss_objectness: 0.0205  loss_rpn_box_reg: 0.0046\n","30/68: lr: 0.0100  loss: 0.2877  loss_classifier: 0.0411  loss_box_reg: 0.0668  loss_mask: 0.1517  loss_objectness: 0.0224  loss_rpn_box_reg: 0.0057\n","40/68: lr: 0.0100  loss: 0.2919  loss_classifier: 0.0389  loss_box_reg: 0.0682  loss_mask: 0.1575  loss_objectness: 0.0216  loss_rpn_box_reg: 0.0057\n","50/68: lr: 0.0100  loss: 0.2823  loss_classifier: 0.0369  loss_box_reg: 0.0655  loss_mask: 0.1533  loss_objectness: 0.0213  loss_rpn_box_reg: 0.0053\n","60/68: lr: 0.0100  loss: 0.2890  loss_classifier: 0.0374  loss_box_reg: 0.0654  loss_mask: 0.1596  loss_objectness: 0.0214  loss_rpn_box_reg: 0.0051\n","68/68: lr: 0.0100  loss: 0.2867  loss_classifier: 0.0369  loss_box_reg: 0.0645  loss_mask: 0.1600  loss_objectness: 0.0204  loss_rpn_box_reg: 0.0050\n","Total time: 19.852, time per item: 0.292\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.298, time per item: 0.126\n","\n","Evaluation Results:\n","mIoU: 0.8699\n","Precision: 0.7662\n","Recall: 1.0000\n","F1: 0.8456\n","mAP: 0.9891\n","Epoch 3: Loss = 0.2867\n","Epoch: [4]\n","10/68: lr: 0.0100  loss: 0.3151  loss_classifier: 0.0392  loss_box_reg: 0.0714  loss_mask: 0.1691  loss_objectness: 0.0284  loss_rpn_box_reg: 0.0069\n","20/68: lr: 0.0100  loss: 0.2824  loss_classifier: 0.0349  loss_box_reg: 0.0567  loss_mask: 0.1610  loss_objectness: 0.0242  loss_rpn_box_reg: 0.0057\n","30/68: lr: 0.0100  loss: 0.2816  loss_classifier: 0.0347  loss_box_reg: 0.0578  loss_mask: 0.1599  loss_objectness: 0.0240  loss_rpn_box_reg: 0.0052\n","40/68: lr: 0.0100  loss: 0.2871  loss_classifier: 0.0372  loss_box_reg: 0.0642  loss_mask: 0.1562  loss_objectness: 0.0240  loss_rpn_box_reg: 0.0055\n","50/68: lr: 0.0100  loss: 0.2875  loss_classifier: 0.0372  loss_box_reg: 0.0629  loss_mask: 0.1580  loss_objectness: 0.0241  loss_rpn_box_reg: 0.0052\n","60/68: lr: 0.0100  loss: 0.2886  loss_classifier: 0.0377  loss_box_reg: 0.0639  loss_mask: 0.1577  loss_objectness: 0.0241  loss_rpn_box_reg: 0.0052\n","68/68: lr: 0.0100  loss: 0.2817  loss_classifier: 0.0361  loss_box_reg: 0.0620  loss_mask: 0.1557  loss_objectness: 0.0229  loss_rpn_box_reg: 0.0050\n","Total time: 19.815, time per item: 0.291\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.347, time per item: 0.128\n","\n","Evaluation Results:\n","mIoU: 0.8705\n","Precision: 0.7655\n","Recall: 1.0000\n","F1: 0.8461\n","mAP: 0.9925\n","Epoch 4: Loss = 0.2817\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.531, time per item: 0.133\n","\n","Evaluation Results:\n","mIoU: 0.8705\n","Precision: 0.7655\n","Recall: 1.0000\n","F1: 0.8461\n","mAP: 0.9925\n","\n","Training with RMSProp optimizer\n","Number of trainable parameters: 273164\n","Epoch: [0]\n","10/68: lr: 0.0010  loss: 6.7330  loss_classifier: 0.1741  loss_box_reg: 0.2905  loss_mask: 6.2370  loss_objectness: 0.0256  loss_rpn_box_reg: 0.0057\n","20/68: lr: 0.0010  loss: 3.6238  loss_classifier: 0.1159  loss_box_reg: 0.1997  loss_mask: 3.2783  loss_objectness: 0.0243  loss_rpn_box_reg: 0.0057\n","30/68: lr: 0.0010  loss: 2.5481  loss_classifier: 0.0930  loss_box_reg: 0.1690  loss_mask: 2.2563  loss_objectness: 0.0245  loss_rpn_box_reg: 0.0053\n","40/68: lr: 0.0010  loss: 2.0035  loss_classifier: 0.0797  loss_box_reg: 0.1492  loss_mask: 1.7469  loss_objectness: 0.0224  loss_rpn_box_reg: 0.0052\n","50/68: lr: 0.0010  loss: 1.6844  loss_classifier: 0.0738  loss_box_reg: 0.1405  loss_mask: 1.4431  loss_objectness: 0.0218  loss_rpn_box_reg: 0.0052\n","60/68: lr: 0.0010  loss: 1.5124  loss_classifier: 0.0690  loss_box_reg: 0.1354  loss_mask: 1.2802  loss_objectness: 0.0227  loss_rpn_box_reg: 0.0052\n","68/68: lr: 0.0010  loss: 1.3753  loss_classifier: 0.0654  loss_box_reg: 0.1283  loss_mask: 1.1545  loss_objectness: 0.0220  loss_rpn_box_reg: 0.0052\n","Total time: 19.760, time per item: 0.291\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.382, time per item: 0.129\n","\n","Evaluation Results:\n","mIoU: 0.8484\n","Precision: 0.7593\n","Recall: 1.0000\n","F1: 0.8396\n","mAP: 0.9854\n","Epoch 0: Loss = 1.3753\n","Epoch: [1]\n","10/68: lr: 0.0010  loss: 0.4449  loss_classifier: 0.0470  loss_box_reg: 0.1036  loss_mask: 0.2649  loss_objectness: 0.0215  loss_rpn_box_reg: 0.0080\n","20/68: lr: 0.0010  loss: 0.3670  loss_classifier: 0.0416  loss_box_reg: 0.0826  loss_mask: 0.2168  loss_objectness: 0.0202  loss_rpn_box_reg: 0.0058\n","30/68: lr: 0.0010  loss: 0.3903  loss_classifier: 0.0387  loss_box_reg: 0.0769  loss_mask: 0.2484  loss_objectness: 0.0211  loss_rpn_box_reg: 0.0052\n","40/68: lr: 0.0010  loss: 0.3730  loss_classifier: 0.0366  loss_box_reg: 0.0742  loss_mask: 0.2370  loss_objectness: 0.0202  loss_rpn_box_reg: 0.0051\n","50/68: lr: 0.0010  loss: 0.3707  loss_classifier: 0.0367  loss_box_reg: 0.0740  loss_mask: 0.2345  loss_objectness: 0.0203  loss_rpn_box_reg: 0.0052\n","60/68: lr: 0.0010  loss: 0.3718  loss_classifier: 0.0385  loss_box_reg: 0.0798  loss_mask: 0.2269  loss_objectness: 0.0214  loss_rpn_box_reg: 0.0052\n","68/68: lr: 0.0010  loss: 0.3648  loss_classifier: 0.0386  loss_box_reg: 0.0790  loss_mask: 0.2198  loss_objectness: 0.0222  loss_rpn_box_reg: 0.0052\n","Total time: 19.969, time per item: 0.294\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.263, time per item: 0.125\n","\n","Evaluation Results:\n","mIoU: 0.8742\n","Precision: 0.8008\n","Recall: 1.0000\n","F1: 0.8702\n","mAP: 0.9945\n","Epoch 1: Loss = 0.3648\n","Epoch: [2]\n","10/68: lr: 0.0010  loss: 0.2930  loss_classifier: 0.0305  loss_box_reg: 0.0687  loss_mask: 0.1718  loss_objectness: 0.0164  loss_rpn_box_reg: 0.0055\n","20/68: lr: 0.0010  loss: 0.3017  loss_classifier: 0.0330  loss_box_reg: 0.0741  loss_mask: 0.1710  loss_objectness: 0.0181  loss_rpn_box_reg: 0.0054\n","30/68: lr: 0.0010  loss: 0.3141  loss_classifier: 0.0360  loss_box_reg: 0.0783  loss_mask: 0.1733  loss_objectness: 0.0206  loss_rpn_box_reg: 0.0059\n","40/68: lr: 0.0010  loss: 0.3345  loss_classifier: 0.0353  loss_box_reg: 0.0721  loss_mask: 0.2008  loss_objectness: 0.0208  loss_rpn_box_reg: 0.0054\n","50/68: lr: 0.0010  loss: 0.3258  loss_classifier: 0.0350  loss_box_reg: 0.0707  loss_mask: 0.1934  loss_objectness: 0.0215  loss_rpn_box_reg: 0.0051\n","60/68: lr: 0.0010  loss: 0.3254  loss_classifier: 0.0361  loss_box_reg: 0.0721  loss_mask: 0.1914  loss_objectness: 0.0206  loss_rpn_box_reg: 0.0052\n","68/68: lr: 0.0010  loss: 0.3241  loss_classifier: 0.0360  loss_box_reg: 0.0733  loss_mask: 0.1889  loss_objectness: 0.0208  loss_rpn_box_reg: 0.0051\n","Total time: 19.983, time per item: 0.294\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.245, time per item: 0.125\n","\n","Evaluation Results:\n","mIoU: 0.8648\n","Precision: 0.8271\n","Recall: 1.0000\n","F1: 0.8882\n","mAP: 0.9957\n","Epoch 2: Loss = 0.3241\n","Epoch: [3]\n","10/68: lr: 0.0010  loss: 0.2534  loss_classifier: 0.0277  loss_box_reg: 0.0632  loss_mask: 0.1355  loss_objectness: 0.0228  loss_rpn_box_reg: 0.0042\n","20/68: lr: 0.0010  loss: 0.3056  loss_classifier: 0.0315  loss_box_reg: 0.0751  loss_mask: 0.1724  loss_objectness: 0.0214  loss_rpn_box_reg: 0.0051\n","30/68: lr: 0.0010  loss: 0.2946  loss_classifier: 0.0317  loss_box_reg: 0.0707  loss_mask: 0.1652  loss_objectness: 0.0225  loss_rpn_box_reg: 0.0046\n","40/68: lr: 0.0010  loss: 0.3072  loss_classifier: 0.0351  loss_box_reg: 0.0736  loss_mask: 0.1702  loss_objectness: 0.0235  loss_rpn_box_reg: 0.0049\n","50/68: lr: 0.0010  loss: 0.3111  loss_classifier: 0.0350  loss_box_reg: 0.0754  loss_mask: 0.1730  loss_objectness: 0.0228  loss_rpn_box_reg: 0.0050\n","60/68: lr: 0.0010  loss: 0.3199  loss_classifier: 0.0349  loss_box_reg: 0.0756  loss_mask: 0.1819  loss_objectness: 0.0224  loss_rpn_box_reg: 0.0050\n","68/68: lr: 0.0010  loss: 0.3314  loss_classifier: 0.0348  loss_box_reg: 0.0749  loss_mask: 0.1948  loss_objectness: 0.0220  loss_rpn_box_reg: 0.0049\n","Total time: 20.327, time per item: 0.299\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.304, time per item: 0.127\n","\n","Evaluation Results:\n","mIoU: 0.8688\n","Precision: 0.8345\n","Recall: 1.0000\n","F1: 0.8942\n","mAP: 0.9957\n","Epoch 3: Loss = 0.3314\n","Epoch: [4]\n","10/68: lr: 0.0010  loss: 0.2890  loss_classifier: 0.0338  loss_box_reg: 0.0765  loss_mask: 0.1496  loss_objectness: 0.0251  loss_rpn_box_reg: 0.0040\n","20/68: lr: 0.0010  loss: 0.2936  loss_classifier: 0.0315  loss_box_reg: 0.0759  loss_mask: 0.1610  loss_objectness: 0.0207  loss_rpn_box_reg: 0.0046\n","30/68: lr: 0.0010  loss: 0.3002  loss_classifier: 0.0325  loss_box_reg: 0.0754  loss_mask: 0.1661  loss_objectness: 0.0212  loss_rpn_box_reg: 0.0050\n","40/68: lr: 0.0010  loss: 0.3755  loss_classifier: 0.0339  loss_box_reg: 0.0780  loss_mask: 0.2360  loss_objectness: 0.0225  loss_rpn_box_reg: 0.0052\n","50/68: lr: 0.0010  loss: 0.3735  loss_classifier: 0.0344  loss_box_reg: 0.0792  loss_mask: 0.2326  loss_objectness: 0.0219  loss_rpn_box_reg: 0.0054\n","60/68: lr: 0.0010  loss: 0.3605  loss_classifier: 0.0345  loss_box_reg: 0.0769  loss_mask: 0.2227  loss_objectness: 0.0210  loss_rpn_box_reg: 0.0054\n","68/68: lr: 0.0010  loss: 0.3534  loss_classifier: 0.0334  loss_box_reg: 0.0735  loss_mask: 0.2201  loss_objectness: 0.0213  loss_rpn_box_reg: 0.0051\n","Total time: 19.594, time per item: 0.288\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.527, time per item: 0.133\n","\n","Evaluation Results:\n","mIoU: 0.8073\n","Precision: 0.8197\n","Recall: 0.9902\n","F1: 0.8795\n","mAP: 0.9859\n","Epoch 4: Loss = 0.3534\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.270, time per item: 0.126\n","\n","Evaluation Results:\n","mIoU: 0.8073\n","Precision: 0.8197\n","Recall: 0.9902\n","F1: 0.8795\n","mAP: 0.9859\n","\n","Training Panoptic FPN\n","\n","Training with Adam optimizer\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Number of trainable parameters: 273164\n","Epoch: [0]\n","10/68: lr: 0.0010  loss: 2.7607  loss_classifier: 0.4383  loss_box_reg: 0.0154  loss_mask: 1.6039  loss_objectness: 0.6857  loss_rpn_box_reg: 0.0174\n","20/68: lr: 0.0010  loss: 2.3193  loss_classifier: 0.3076  loss_box_reg: 0.0136  loss_mask: 1.2926  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0197\n","30/68: lr: 0.0010  loss: 2.0927  loss_classifier: 0.2360  loss_box_reg: 0.0140  loss_mask: 1.1358  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0212\n","40/68: lr: 0.0010  loss: 1.9471  loss_classifier: 0.1941  loss_box_reg: 0.0133  loss_mask: 1.0322  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0217\n","50/68: lr: 0.0010  loss: 1.8442  loss_classifier: 0.1671  loss_box_reg: 0.0137  loss_mask: 0.9560  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0216\n","60/68: lr: 0.0010  loss: 1.7726  loss_classifier: 0.1473  loss_box_reg: 0.0126  loss_mask: 0.9042  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0227\n","68/68: lr: 0.0010  loss: 1.7366  loss_classifier: 0.1371  loss_box_reg: 0.0130  loss_mask: 0.8764  loss_objectness: 0.6859  loss_rpn_box_reg: 0.0242\n","Total time: 18.694, time per item: 0.275\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 5.423, time per item: 0.159\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 0: Loss = 1.7366\n","Epoch: [1]\n","10/68: lr: 0.0010  loss: 1.3661  loss_classifier: 0.0442  loss_box_reg: 0.0080  loss_mask: 0.6082  loss_objectness: 0.6860  loss_rpn_box_reg: 0.0197\n","20/68: lr: 0.0010  loss: 1.3810  loss_classifier: 0.0483  loss_box_reg: 0.0102  loss_mask: 0.6088  loss_objectness: 0.6865  loss_rpn_box_reg: 0.0273\n","30/68: lr: 0.0010  loss: 1.3606  loss_classifier: 0.0481  loss_box_reg: 0.0104  loss_mask: 0.5895  loss_objectness: 0.6863  loss_rpn_box_reg: 0.0264\n","40/68: lr: 0.0010  loss: 1.3765  loss_classifier: 0.0488  loss_box_reg: 0.0118  loss_mask: 0.6035  loss_objectness: 0.6862  loss_rpn_box_reg: 0.0263\n","50/68: lr: 0.0010  loss: 1.3825  loss_classifier: 0.0491  loss_box_reg: 0.0125  loss_mask: 0.6096  loss_objectness: 0.6862  loss_rpn_box_reg: 0.0250\n","60/68: lr: 0.0010  loss: 1.3852  loss_classifier: 0.0483  loss_box_reg: 0.0123  loss_mask: 0.6135  loss_objectness: 0.6861  loss_rpn_box_reg: 0.0251\n","68/68: lr: 0.0010  loss: 1.3759  loss_classifier: 0.0471  loss_box_reg: 0.0122  loss_mask: 0.6064  loss_objectness: 0.6860  loss_rpn_box_reg: 0.0242\n","Total time: 18.618, time per item: 0.274\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.647, time per item: 0.137\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 1: Loss = 1.3759\n","Epoch: [2]\n","10/68: lr: 0.0010  loss: 1.3710  loss_classifier: 0.0448  loss_box_reg: 0.0121  loss_mask: 0.5989  loss_objectness: 0.6861  loss_rpn_box_reg: 0.0291\n","20/68: lr: 0.0010  loss: 1.3508  loss_classifier: 0.0455  loss_box_reg: 0.0111  loss_mask: 0.5768  loss_objectness: 0.6863  loss_rpn_box_reg: 0.0312\n","30/68: lr: 0.0010  loss: 1.3431  loss_classifier: 0.0446  loss_box_reg: 0.0109  loss_mask: 0.5705  loss_objectness: 0.6861  loss_rpn_box_reg: 0.0311\n","40/68: lr: 0.0010  loss: 1.3302  loss_classifier: 0.0440  loss_box_reg: 0.0117  loss_mask: 0.5607  loss_objectness: 0.6859  loss_rpn_box_reg: 0.0280\n","50/68: lr: 0.0010  loss: 1.3348  loss_classifier: 0.0432  loss_box_reg: 0.0114  loss_mask: 0.5666  loss_objectness: 0.6860  loss_rpn_box_reg: 0.0276\n","60/68: lr: 0.0010  loss: 1.3438  loss_classifier: 0.0432  loss_box_reg: 0.0112  loss_mask: 0.5771  loss_objectness: 0.6860  loss_rpn_box_reg: 0.0263\n","68/68: lr: 0.0010  loss: 1.3467  loss_classifier: 0.0429  loss_box_reg: 0.0115  loss_mask: 0.5814  loss_objectness: 0.6859  loss_rpn_box_reg: 0.0251\n","Total time: 18.615, time per item: 0.274\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.183, time per item: 0.123\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 2: Loss = 1.3467\n","Epoch: [3]\n","10/68: lr: 0.0010  loss: 1.3599  loss_classifier: 0.0342  loss_box_reg: 0.0085  loss_mask: 0.6111  loss_objectness: 0.6857  loss_rpn_box_reg: 0.0204\n","20/68: lr: 0.0010  loss: 1.3288  loss_classifier: 0.0383  loss_box_reg: 0.0112  loss_mask: 0.5738  loss_objectness: 0.6856  loss_rpn_box_reg: 0.0199\n","30/68: lr: 0.0010  loss: 1.3230  loss_classifier: 0.0388  loss_box_reg: 0.0114  loss_mask: 0.5673  loss_objectness: 0.6856  loss_rpn_box_reg: 0.0199\n","40/68: lr: 0.0010  loss: 1.3342  loss_classifier: 0.0434  loss_box_reg: 0.0139  loss_mask: 0.5688  loss_objectness: 0.6856  loss_rpn_box_reg: 0.0224\n","50/68: lr: 0.0010  loss: 1.3315  loss_classifier: 0.0431  loss_box_reg: 0.0131  loss_mask: 0.5645  loss_objectness: 0.6857  loss_rpn_box_reg: 0.0251\n","60/68: lr: 0.0010  loss: 1.3287  loss_classifier: 0.0426  loss_box_reg: 0.0134  loss_mask: 0.5623  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0245\n","68/68: lr: 0.0010  loss: 1.3218  loss_classifier: 0.0422  loss_box_reg: 0.0133  loss_mask: 0.5566  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0239\n","Total time: 18.935, time per item: 0.278\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.205, time per item: 0.124\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 3: Loss = 1.3218\n","Epoch: [4]\n","10/68: lr: 0.0010  loss: 1.2887  loss_classifier: 0.0375  loss_box_reg: 0.0126  loss_mask: 0.5263  loss_objectness: 0.6864  loss_rpn_box_reg: 0.0259\n","20/68: lr: 0.0010  loss: 1.3127  loss_classifier: 0.0420  loss_box_reg: 0.0150  loss_mask: 0.5468  loss_objectness: 0.6860  loss_rpn_box_reg: 0.0230\n","30/68: lr: 0.0010  loss: 1.3164  loss_classifier: 0.0426  loss_box_reg: 0.0146  loss_mask: 0.5482  loss_objectness: 0.6860  loss_rpn_box_reg: 0.0250\n","40/68: lr: 0.0010  loss: 1.3263  loss_classifier: 0.0415  loss_box_reg: 0.0139  loss_mask: 0.5598  loss_objectness: 0.6859  loss_rpn_box_reg: 0.0251\n","50/68: lr: 0.0010  loss: 1.3281  loss_classifier: 0.0417  loss_box_reg: 0.0131  loss_mask: 0.5619  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0255\n","60/68: lr: 0.0010  loss: 1.3195  loss_classifier: 0.0404  loss_box_reg: 0.0128  loss_mask: 0.5551  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0253\n","68/68: lr: 0.0010  loss: 1.3121  loss_classifier: 0.0393  loss_box_reg: 0.0125  loss_mask: 0.5501  loss_objectness: 0.6858  loss_rpn_box_reg: 0.0244\n","Total time: 18.892, time per item: 0.278\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.428, time per item: 0.130\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 4: Loss = 1.3121\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.240, time per item: 0.125\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","\n","Training with AdaGrad optimizer\n","Number of trainable parameters: 273164\n","Epoch: [0]\n","10/68: lr: 0.0100  loss: 5.7889  loss_classifier: 0.1588  loss_box_reg: 0.0228  loss_mask: 4.8830  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0301\n","20/68: lr: 0.0100  loss: 3.6638  loss_classifier: 0.1098  loss_box_reg: 0.0227  loss_mask: 2.8093  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0280\n","30/68: lr: 0.0100  loss: 2.9428  loss_classifier: 0.0914  loss_box_reg: 0.0235  loss_mask: 2.1090  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0249\n","40/68: lr: 0.0100  loss: 2.5585  loss_classifier: 0.0826  loss_box_reg: 0.0238  loss_mask: 1.7337  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0243\n","50/68: lr: 0.0100  loss: 2.3258  loss_classifier: 0.0760  loss_box_reg: 0.0237  loss_mask: 1.5077  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0245\n","60/68: lr: 0.0100  loss: 2.1694  loss_classifier: 0.0718  loss_box_reg: 0.0233  loss_mask: 1.3565  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0239\n","68/68: lr: 0.0100  loss: 2.0756  loss_classifier: 0.0687  loss_box_reg: 0.0233  loss_mask: 1.2661  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0235\n","Total time: 19.001, time per item: 0.279\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.399, time per item: 0.129\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 0: Loss = 2.0756\n","Epoch: [1]\n","10/68: lr: 0.0100  loss: 1.3901  loss_classifier: 0.0533  loss_box_reg: 0.0290  loss_mask: 0.5926  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0212\n","20/68: lr: 0.0100  loss: 1.3829  loss_classifier: 0.0506  loss_box_reg: 0.0256  loss_mask: 0.5921  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0207\n","30/68: lr: 0.0100  loss: 1.3786  loss_classifier: 0.0487  loss_box_reg: 0.0241  loss_mask: 0.5874  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0244\n","40/68: lr: 0.0100  loss: 1.3688  loss_classifier: 0.0453  loss_box_reg: 0.0219  loss_mask: 0.5829  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0248\n","50/68: lr: 0.0100  loss: 1.3640  loss_classifier: 0.0454  loss_box_reg: 0.0223  loss_mask: 0.5790  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0232\n","60/68: lr: 0.0100  loss: 1.3553  loss_classifier: 0.0444  loss_box_reg: 0.0218  loss_mask: 0.5717  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0233\n","68/68: lr: 0.0100  loss: 1.3597  loss_classifier: 0.0442  loss_box_reg: 0.0211  loss_mask: 0.5762  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0240\n","Total time: 18.618, time per item: 0.274\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.661, time per item: 0.137\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 1: Loss = 1.3597\n","Epoch: [2]\n","10/68: lr: 0.0100  loss: 1.2992  loss_classifier: 0.0381  loss_box_reg: 0.0172  loss_mask: 0.5235  loss_objectness: 0.6938  loss_rpn_box_reg: 0.0265\n","20/68: lr: 0.0100  loss: 1.3167  loss_classifier: 0.0398  loss_box_reg: 0.0180  loss_mask: 0.5405  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0243\n","30/68: lr: 0.0100  loss: 1.3113  loss_classifier: 0.0388  loss_box_reg: 0.0187  loss_mask: 0.5373  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0223\n","40/68: lr: 0.0100  loss: 1.3118  loss_classifier: 0.0400  loss_box_reg: 0.0193  loss_mask: 0.5354  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0230\n","50/68: lr: 0.0100  loss: 1.3145  loss_classifier: 0.0392  loss_box_reg: 0.0187  loss_mask: 0.5388  loss_objectness: 0.6942  loss_rpn_box_reg: 0.0236\n","60/68: lr: 0.0100  loss: 1.3152  loss_classifier: 0.0400  loss_box_reg: 0.0195  loss_mask: 0.5380  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0236\n","68/68: lr: 0.0100  loss: 1.3187  loss_classifier: 0.0402  loss_box_reg: 0.0201  loss_mask: 0.5409  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0234\n","Total time: 18.479, time per item: 0.272\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.395, time per item: 0.129\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 2: Loss = 1.3187\n","Epoch: [3]\n","10/68: lr: 0.0100  loss: 1.2957  loss_classifier: 0.0356  loss_box_reg: 0.0177  loss_mask: 0.5213  loss_objectness: 0.6944  loss_rpn_box_reg: 0.0267\n","20/68: lr: 0.0100  loss: 1.2946  loss_classifier: 0.0381  loss_box_reg: 0.0197  loss_mask: 0.5186  loss_objectness: 0.6942  loss_rpn_box_reg: 0.0239\n","30/68: lr: 0.0100  loss: 1.3055  loss_classifier: 0.0400  loss_box_reg: 0.0226  loss_mask: 0.5250  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0238\n","40/68: lr: 0.0100  loss: 1.2999  loss_classifier: 0.0375  loss_box_reg: 0.0209  loss_mask: 0.5258  loss_objectness: 0.6940  loss_rpn_box_reg: 0.0218\n","50/68: lr: 0.0100  loss: 1.3000  loss_classifier: 0.0368  loss_box_reg: 0.0208  loss_mask: 0.5256  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0226\n","60/68: lr: 0.0100  loss: 1.2989  loss_classifier: 0.0372  loss_box_reg: 0.0204  loss_mask: 0.5246  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0226\n","68/68: lr: 0.0100  loss: 1.3017  loss_classifier: 0.0372  loss_box_reg: 0.0201  loss_mask: 0.5263  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0239\n","Total time: 18.926, time per item: 0.278\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.378, time per item: 0.129\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 3: Loss = 1.3017\n","Epoch: [4]\n","10/68: lr: 0.0100  loss: 1.2620  loss_classifier: 0.0319  loss_box_reg: 0.0169  loss_mask: 0.5001  loss_objectness: 0.6943  loss_rpn_box_reg: 0.0188\n","20/68: lr: 0.0100  loss: 1.2721  loss_classifier: 0.0324  loss_box_reg: 0.0161  loss_mask: 0.5058  loss_objectness: 0.6942  loss_rpn_box_reg: 0.0236\n","30/68: lr: 0.0100  loss: 1.2884  loss_classifier: 0.0351  loss_box_reg: 0.0180  loss_mask: 0.5171  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0242\n","40/68: lr: 0.0100  loss: 1.2906  loss_classifier: 0.0358  loss_box_reg: 0.0186  loss_mask: 0.5190  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0230\n","50/68: lr: 0.0100  loss: 1.2922  loss_classifier: 0.0369  loss_box_reg: 0.0199  loss_mask: 0.5193  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0220\n","60/68: lr: 0.0100  loss: 1.2874  loss_classifier: 0.0361  loss_box_reg: 0.0191  loss_mask: 0.5157  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0225\n","68/68: lr: 0.0100  loss: 1.2887  loss_classifier: 0.0359  loss_box_reg: 0.0199  loss_mask: 0.5153  loss_objectness: 0.6941  loss_rpn_box_reg: 0.0235\n","Total time: 18.974, time per item: 0.279\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.526, time per item: 0.133\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 4: Loss = 1.2887\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.384, time per item: 0.129\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","\n","Training with RMSProp optimizer\n","Number of trainable parameters: 273164\n","Epoch: [0]\n","10/68: lr: 0.0010  loss: 6.1051  loss_classifier: 0.1650  loss_box_reg: 0.0310  loss_mask: 5.1937  loss_objectness: 0.6955  loss_rpn_box_reg: 0.0199\n","20/68: lr: 0.0010  loss: 3.8109  loss_classifier: 0.1141  loss_box_reg: 0.0291  loss_mask: 2.9479  loss_objectness: 0.6948  loss_rpn_box_reg: 0.0250\n","30/68: lr: 0.0010  loss: 3.0460  loss_classifier: 0.0960  loss_box_reg: 0.0298  loss_mask: 2.2000  loss_objectness: 0.6951  loss_rpn_box_reg: 0.0250\n","40/68: lr: 0.0010  loss: 2.6449  loss_classifier: 0.0871  loss_box_reg: 0.0304  loss_mask: 1.8080  loss_objectness: 0.6950  loss_rpn_box_reg: 0.0244\n","50/68: lr: 0.0010  loss: 2.4064  loss_classifier: 0.0810  loss_box_reg: 0.0305  loss_mask: 1.5739  loss_objectness: 0.6950  loss_rpn_box_reg: 0.0259\n","60/68: lr: 0.0010  loss: 2.2325  loss_classifier: 0.0754  loss_box_reg: 0.0293  loss_mask: 1.4079  loss_objectness: 0.6950  loss_rpn_box_reg: 0.0248\n","68/68: lr: 0.0010  loss: 2.1302  loss_classifier: 0.0723  loss_box_reg: 0.0292  loss_mask: 1.3101  loss_objectness: 0.6951  loss_rpn_box_reg: 0.0236\n","Total time: 18.963, time per item: 0.279\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.662, time per item: 0.137\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 0: Loss = 2.1302\n","Epoch: [1]\n","10/68: lr: 0.0010  loss: 1.4024  loss_classifier: 0.0498  loss_box_reg: 0.0330  loss_mask: 0.5979  loss_objectness: 0.6946  loss_rpn_box_reg: 0.0270\n","20/68: lr: 0.0010  loss: 1.3751  loss_classifier: 0.0490  loss_box_reg: 0.0321  loss_mask: 0.5707  loss_objectness: 0.6950  loss_rpn_box_reg: 0.0284\n","30/68: lr: 0.0010  loss: 1.3755  loss_classifier: 0.0474  loss_box_reg: 0.0311  loss_mask: 0.5768  loss_objectness: 0.6953  loss_rpn_box_reg: 0.0250\n","40/68: lr: 0.0010  loss: 1.3729  loss_classifier: 0.0462  loss_box_reg: 0.0301  loss_mask: 0.5757  loss_objectness: 0.6953  loss_rpn_box_reg: 0.0256\n","50/68: lr: 0.0010  loss: 1.3714  loss_classifier: 0.0468  loss_box_reg: 0.0307  loss_mask: 0.5732  loss_objectness: 0.6953  loss_rpn_box_reg: 0.0253\n","60/68: lr: 0.0010  loss: 1.3613  loss_classifier: 0.0451  loss_box_reg: 0.0292  loss_mask: 0.5680  loss_objectness: 0.6953  loss_rpn_box_reg: 0.0237\n","68/68: lr: 0.0010  loss: 1.3592  loss_classifier: 0.0452  loss_box_reg: 0.0290  loss_mask: 0.5664  loss_objectness: 0.6954  loss_rpn_box_reg: 0.0232\n","Total time: 18.642, time per item: 0.274\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.870, time per item: 0.143\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 1: Loss = 1.3592\n","Epoch: [2]\n","10/68: lr: 0.0010  loss: 1.3282  loss_classifier: 0.0371  loss_box_reg: 0.0233  loss_mask: 0.5455  loss_objectness: 0.6952  loss_rpn_box_reg: 0.0272\n","20/68: lr: 0.0010  loss: 1.3197  loss_classifier: 0.0364  loss_box_reg: 0.0239  loss_mask: 0.5404  loss_objectness: 0.6954  loss_rpn_box_reg: 0.0236\n","30/68: lr: 0.0010  loss: 1.3203  loss_classifier: 0.0388  loss_box_reg: 0.0258  loss_mask: 0.5370  loss_objectness: 0.6953  loss_rpn_box_reg: 0.0234\n","40/68: lr: 0.0010  loss: 1.3177  loss_classifier: 0.0390  loss_box_reg: 0.0255  loss_mask: 0.5337  loss_objectness: 0.6955  loss_rpn_box_reg: 0.0240\n","50/68: lr: 0.0010  loss: 1.3174  loss_classifier: 0.0388  loss_box_reg: 0.0263  loss_mask: 0.5320  loss_objectness: 0.6953  loss_rpn_box_reg: 0.0249\n","60/68: lr: 0.0010  loss: 1.3248  loss_classifier: 0.0384  loss_box_reg: 0.0266  loss_mask: 0.5405  loss_objectness: 0.6952  loss_rpn_box_reg: 0.0241\n","68/68: lr: 0.0010  loss: 1.3228  loss_classifier: 0.0375  loss_box_reg: 0.0264  loss_mask: 0.5402  loss_objectness: 0.6952  loss_rpn_box_reg: 0.0235\n","Total time: 18.750, time per item: 0.276\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 5.026, time per item: 0.148\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 2: Loss = 1.3228\n","Epoch: [3]\n","10/68: lr: 0.0010  loss: 1.3224  loss_classifier: 0.0407  loss_box_reg: 0.0289  loss_mask: 0.5325  loss_objectness: 0.6953  loss_rpn_box_reg: 0.0250\n","20/68: lr: 0.0010  loss: 1.3106  loss_classifier: 0.0394  loss_box_reg: 0.0281  loss_mask: 0.5201  loss_objectness: 0.6954  loss_rpn_box_reg: 0.0276\n","30/68: lr: 0.0010  loss: 1.3117  loss_classifier: 0.0378  loss_box_reg: 0.0283  loss_mask: 0.5249  loss_objectness: 0.6953  loss_rpn_box_reg: 0.0253\n","40/68: lr: 0.0010  loss: 1.3036  loss_classifier: 0.0362  loss_box_reg: 0.0276  loss_mask: 0.5208  loss_objectness: 0.6951  loss_rpn_box_reg: 0.0240\n","50/68: lr: 0.0010  loss: 1.3124  loss_classifier: 0.0370  loss_box_reg: 0.0279  loss_mask: 0.5274  loss_objectness: 0.6951  loss_rpn_box_reg: 0.0250\n","60/68: lr: 0.0010  loss: 1.3085  loss_classifier: 0.0358  loss_box_reg: 0.0271  loss_mask: 0.5260  loss_objectness: 0.6952  loss_rpn_box_reg: 0.0245\n","68/68: lr: 0.0010  loss: 1.3055  loss_classifier: 0.0350  loss_box_reg: 0.0271  loss_mask: 0.5246  loss_objectness: 0.6952  loss_rpn_box_reg: 0.0236\n","Total time: 19.013, time per item: 0.280\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.698, time per item: 0.138\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 3: Loss = 1.3055\n","Epoch: [4]\n","10/68: lr: 0.0010  loss: 1.3060  loss_classifier: 0.0397  loss_box_reg: 0.0267  loss_mask: 0.5191  loss_objectness: 0.6963  loss_rpn_box_reg: 0.0242\n","20/68: lr: 0.0010  loss: 1.3113  loss_classifier: 0.0360  loss_box_reg: 0.0260  loss_mask: 0.5275  loss_objectness: 0.6958  loss_rpn_box_reg: 0.0260\n","30/68: lr: 0.0010  loss: 1.3069  loss_classifier: 0.0354  loss_box_reg: 0.0290  loss_mask: 0.5226  loss_objectness: 0.6956  loss_rpn_box_reg: 0.0244\n","40/68: lr: 0.0010  loss: 1.3079  loss_classifier: 0.0354  loss_box_reg: 0.0289  loss_mask: 0.5229  loss_objectness: 0.6954  loss_rpn_box_reg: 0.0254\n","50/68: lr: 0.0010  loss: 1.3000  loss_classifier: 0.0336  loss_box_reg: 0.0270  loss_mask: 0.5199  loss_objectness: 0.6954  loss_rpn_box_reg: 0.0240\n","60/68: lr: 0.0010  loss: 1.2998  loss_classifier: 0.0321  loss_box_reg: 0.0263  loss_mask: 0.5221  loss_objectness: 0.6954  loss_rpn_box_reg: 0.0239\n","68/68: lr: 0.0010  loss: 1.2972  loss_classifier: 0.0328  loss_box_reg: 0.0262  loss_mask: 0.5194  loss_objectness: 0.6953  loss_rpn_box_reg: 0.0236\n","Total time: 19.059, time per item: 0.280\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.728, time per item: 0.139\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n","Epoch 4: Loss = 1.2972\n","Test:\n","10/34: \n","20/34: \n","30/34: \n","34/34: \n","Total time: 4.944, time per item: 0.145\n","\n","Evaluation Results:\n","mIoU: 0.0000\n","Precision: 0.0000\n","Recall: 0.0000\n","F1: 0.0000\n","mAP: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-38-e5b39c7a588f>:128: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n","  plt.ylim(0, max(metric_data) * 1.2)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results and visualizations saved to /content/drive/MyDrive/DL-projet/results/instance_segmentation_results\n"]}]},{"cell_type":"markdown","source":["https://arxiv.org/pdf/2104.11892"],"metadata":{"id":"66HB_OM_bNHq"}}]}